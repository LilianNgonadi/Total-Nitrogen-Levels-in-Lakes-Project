---
author: "Lilian Ngonadi"
date: "2024-02-03"
rmarkdown::github_document
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Libraries
```{r echo=TRUE}
library(tidyverse)
library(caret)
library(dplyr)
library(fastDummies)
library(corrplot)
library(RColorBrewer)
library(ggplot2)
library(reshape2)
library(gridExtra)
library(car)
library(gvlma)
library(ipred)
library(GGally)
library(MASS)
library(caret)
library(leaps)
library(gt)
library(glmnet)
library(ISLR)
library(pls)
library(Metrics)
library(pls)
```

## Data Preparation


```{r load, echo=TRUE}
load("/Users/lilianngonadi/Downloads/lakeNitrogen(2).Rdata")
```

## To load my lakeNitrogen data as data frame
```{r echo=TRUE}
data<-as.data.frame(lakeNitrogen)
```

```{r}
X = model.matrix(log(TN) ~ Baseflow + NO3Depo + TotalDepo + Runoff + sqrt(Urban) + sqrt(Rowcrop) + sqrt(Pasture) + Forest + sqrt(Wetland) + log(LakeArea) + log(MaxDepth) + log(LWR) + Connectivity,data = data)[,-1]
```

## 1. Fit Ridge Regression
```{r}
fit.ridge=glmnet(X,log(data$TN) ,alpha=0,lambda=exp(seq(-2,6,length.out=100)),nlambda=100)
plot(fit.ridge,label=TRUE,xvar="lambda")
```
##  L1 Norm
```{r}
plot(fit.ridge,label=TRUE,xvar="norm")
```

##  Fraction Deviance Explained plot
```{r}
plot(fit.ridge,label=TRUE,xvar="dev")
```
# Scaling
```{r}
lambda = fit.ridge$lambda # vector of lambda values used in solution path 
betahat = fit.ridge$beta # matrix of order nvar by length(lambda)
nn = nrow(X)
Xsd = apply(X,2,sd) *sqrt((nn-1)/nn); Xsd # the scaling applied to columns of X
```

```{r}
ysd = sd(log(data$TN)) *sqrt((nn-1)/nn); ysd # the scaling applied to Y
```

# mimic Fig 6.4 from ISLR: plot predictor-standardized coefficients vs. lambda (on the log scale)
```{r}
par(xpd=TRUE)
matplot(lambda, t(betahat * Xsd), type="l", log="x", lwd=3,lty=1:5,col=1:6) 
legend("bottomright", legend=names(Xsd), 
       horiz=FALSE, lty=1:5, col=1:6, cex=0.5, xpd=TRUE, xjust=1, yjust=0)
axis(3, at=lambda, labels=fit.ridge$df, xpd=TRUE)
```


# mimic Fig 6.4 from ISLR: plot predictor-standardized coefficients vs. L2 norm of vector scaled to OLS fit
```{r}
betaOLS = coef(lm(log(data$TN)~X))[-1]
L2norm = function(x) sqrt(sum(x^2))
betaL2.scaled = apply( betahat, 2, L2norm ) / L2norm(betaOLS)
matplot(betaL2.scaled, t(betahat * Xsd), type="l", lwd=2,lty=1:5,col=1:6)
legend("bottomleft",legend=names(Xsd), 
       horiz=FALSE, lty=1:5, col=1:6, cex=0.4)
axis(3, at=lambda, labels=fit.ridge$df)
```
# Ridge Regression Cross-Validation Plot"
```{r}
set.seed(732) 
cv.ridge =
  cv.glmnet(X,log(data$TN),alpha=0,lambda=exp(seq(-2,6,length.out=100)),nlambda=100) 
plot(cv.ridge)
```

```{r}
cv.ridge$lambda.min
```

```{r}
cv.ridge$lambda.1se
```

# Ridge Model fit
```{r}
best_model <- glmnet(X,log(data$TN),alpha=0,lambda =cv.ridge$lambda.1se)
coef(best_model)
```
```{r}
mean(predict(cv.ridge,X,s=cv.ridge$lambda.1se) - log(data$TN)^2)
```

# performance metrics
```{r}
# Predict using the ridge regression model
predictions <- predict(cv.ridge, s = cv.ridge$lambda.1se, newx = X)


# Calculate R-squared
r_squared <- cor(log(data$TN), predictions)^2

# Calculate MSE
mse <- mean((log(data$TN) - predictions)^2)

# Output the metrics
print(paste("R-squared:", r_squared))
print(paste("MSE:", mse))

# Other performance metrics can also be calculated
mae <- mean(abs(log(data$TN) - predictions)) # Mean Absolute Error
rmse <- sqrt(mse) # Root Mean Squared Error

# Print other metrics
print(paste("MAE:", mae))
print(paste("RMSE:", rmse))
```

# 2. Fit Lasso

```{r}
fit.lasso = glmnet(X,log(data$TN),alpha=1 ) 
plot(fit.lasso,label=TRUE,xvar="lambda")
```

```{r}
lambda = fit.lasso$lambda # vector of lambda values used in solution path
betahat = fit.lasso$beta # matrix of order nvar by length(lambda)
```

# mimic Fig 6.6 from ISLR: plot predictor-standardized coefficients vs. lambda (on the log scale)

```{r}
matplot(lambda, t(betahat * Xsd), type="l", log="x", lwd=2, lty=1:5, col=1:6); 
legend("bottomright", inset=.01, legend=names(Xsd), horiz=FALSE, lty=1:5, col=1:6, cex=.3); 
axis(3, at=lambda, labels=fit.lasso$df)
```

# mimic Fig 6.6 from ISLR: plot predictor-standardized coefficients vs. L1 norm of vector scaled to OLS fit
```{r}
L1norm = function(x) sum(abs(x))
betaL1.scaled = apply( betahat, 2, L1norm ) / L1norm(betaOLS)
matplot(betaL1.scaled, t(betahat * Xsd), type="l", lwd=2,lty=1:5,col=1:6)
legend("bottomleft",inset=.01,legend=names(Xsd),horiz=FALSE,lty=1:5,col=1:6,cex=.3)
axis(3, at=betaL1.scaled, labels=fit.lasso$df)
```
# Lasso Regression Cross-Validation Plot"

```{r}
set.seed(732)
cv.lasso = cv.glmnet(X,log(data$TN),alpha=1) 
plot(cv.lasso)
```

```{r}
cv.lasso$lambda.min
```

```{r}
cv.lasso$lambda.1se
```

# Lasso Model fit
```{r}
best_model1 <- glmnet(X,log(data$TN),alpha=1,lambda =cv.lasso$lambda.1se)
coef(best_model1)
```

# performance metrics
```{r}
# Predict using the ridge regression model
predictions <- predict(cv.lasso, s = cv.lasso$lambda.1se, newx = X)


# Calculate R-squared
r_squared <- cor(log(data$TN), predictions)^2

# Calculate MSE
mse <- mean((log(data$TN) - predictions)^2)

# Output the metrics
print(paste("R-squared:", r_squared))
print(paste("MSE:", mse))

# Other performance metrics can also be calculated
mae <- mean(abs(log(data$TN) - predictions)) # Mean Absolute Error
rmse <- sqrt(mse) # Root Mean Squared Error

# Print other metrics
print(paste("MAE:", mae))
print(paste("RMSE:", rmse))
```
## 3. Principal component regression

```{r}
set.seed(732)
fit.pcr = pcr(log(TN) ~ Baseflow + NO3Depo + TotalDepo + Runoff + sqrt(Urban) + sqrt(Rowcrop) + sqrt(Pasture) + Forest + sqrt(Wetland) + log(LakeArea) + log(MaxDepth) + log(LWR) + Connectivity,data=data,scale=TRUE,validation="CV",
segments=10,segment.type="random") 
summary(fit.pcr)
```

```{r}
loadings_pcr <- loadings(fit.pcr)
# Print the loadings
print(loadings_pcr[, 1:7])
```
```{r}
# Summary of the fit to access overall statistics
summary(fit.pcr)

# Accessing explained variance for each component
explained_variance <- fit.pcr$validation$Tvar # Total variance explained by each component
eigenvalues <- fit.pcr$scores^2 / nrow(fit.pcr$scores) # Eigenvalues for each component

# Print eigenvalues of the first seven components
cat("Eigenvalues of the First Seven Components:\n")
print(eigenvalues[1:7])
```
```{r}

# Calculate the variance explained by each principal component
var_explained <- fit.pcr$sdev^2
total_variance <- sum(var_explained)
percentage_var_explained <- 100 * var_explained / total_variance

# Calculate cumulative percentage of explained variance
cumulative_percentage_var_explained <- cumsum(percentage_var_explained)

# Print the percentage of variance explained by each component
print(percentage_var_explained)

# Print the cumulative percentage of variance explained
print(cumulative_percentage_var_explained)
```

```{r}
# Adjust global text size, affecting legends among other text elements
par(cex=0.5) # Adjust this value as needed, smaller values for smaller text

# Your plotting function here
# Assuming 'oefplot' is your plotting command:
coefplot(fit.pcr, separate=FALSE, ncomp=1:14, legendpos="bottomright")
```

```{r}
ncomp.pcr1se<-selectNcomp(fit.pcr,method="onesigma",plot=TRUE)
```

```{r}
validationplot(fit.pcr, val.type = "MSEP")
```

```{r}
optimal_ncomp <- which.min(RMSEP(fit.pcr)$val)
pcr_best <- pcr(log(TN) ~ Baseflow + NO3Depo + TotalDepo + Runoff + sqrt(Urban) + sqrt(Rowcrop) + sqrt(Pasture) + Forest + sqrt(Wetland) + log(LakeArea) + log(MaxDepth) + log(LWR) + Connectivity, data = data, scale = TRUE, ncomp = 7)
coef(pcr_best)
```

```{r}
# Predict using the PCR regression model
predictions <- predict(fit.pcr,newdata=data,ncomp=ncomp.pcr1se)


# Calculate R-squared
r_squared <- cor(log(data$TN), predictions)^2

# Calculate MSE
mse <- mean((log(data$TN) - predictions)^2)

# Output the metrics
print(paste("R-squared:", r_squared))
print(paste("MSE:", mse))

# Other performance metrics can also be calculated
mae <- mean(abs(log(data$TN) - predictions)) # Mean Absolute Error
rmse <- sqrt(mse) # Root Mean Squared Error

# Print other metrics
print(paste("MAE:", mae))
print(paste("RMSE:", rmse))
```

## 4. Partial least squares

```{r}
fit.pls = plsr(log(TN) ~ Baseflow + NO3Depo + TotalDepo + Runoff + sqrt(Urban) + sqrt(Rowcrop) + sqrt(Pasture) + Forest + sqrt(Wetland) + log(LakeArea) + log(MaxDepth) + log(LWR) + Connectivity,data=data,scale=TRUE,validation="CV",
segments=10,segment.type="random",method="simpls") 
summary(fit.pls)
```

```{r}
# Adjust global text size, affecting legends among other text elements
par(cex=0.5) # Adjust this value as needed, smaller values for smaller text

# Your plotting function here
# Assuming 'oefplot' is your plotting command:
coefplot(fit.pls,separate=FALSE,ncomp=1:14,legendpos="bottomright")
```

```{r}
ncomp.pls1se = selectNcomp(fit.pls,method="onesigma",plot=TRUE)
```

```{r}
ncomp.pls1se
```

```{r}
# Predict using the ridge regression model
predictions <- predict(fit.pls,newdata=data,ncomp=ncomp.pcr1se)


# Calculate R-squared
r_squared <- cor(log(data$TN), predictions)^2

# Calculate MSE
mse <- mean((log(data$TN) - predictions)^2)

# Output the metrics
print(paste("R-squared:", r_squared))
print(paste("MSE:", mse))

# Other performance metrics can also be calculated
mae <- mean(abs(log(data$TN) - predictions)) # Mean Absolute Error
rmse <- sqrt(mse) # Root Mean Squared Error

# Print other metrics
print(paste("MAE:", mae))
print(paste("RMSE:", rmse))
```

Here we can compare the two, PLS and PCR, based on their coefficient estimates as well as their MSEP.
```{r}
cbind(coef(fit.pcr,ncomp=ncomp.pcr1se), coef(fit.pls,ncomp=ncomp.pls1se))
```

```{r}
mean( (predict(fit.pcr,newdata=data,ncomp=ncomp.pcr1se,type="response") - + data$TN)^2)
```

```{r}
 mean( (predict(fit.pls,newdata=data,ncomp=ncomp.pls1se,type="response") - + data$TN)^2)
```
